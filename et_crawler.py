# et_crawler.py
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime  # âœ… åŠ å…¥ datetime ç”¨ä¾†è½‰æ›æ™‚é–“æ ¼å¼

# è‡ªè¨‚ headers é¿å…è¢«ç¶²ç«™é˜»æ“‹
headers = {"User-Agent": "Mozilla/5.0"}

def parse_article(url):
    """è§£æå–®ç¯‡æ–°èæ–‡ç« å…§å®¹"""
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        # å–å¾—æ¨™é¡Œ
        title = soup.select_one("h1.title")

        # å–å¾—æ™‚é–“æ¨™ç±¤ä¸¦è½‰æ›æ ¼å¼ï¼ˆä¾‹å¦‚ï¼š'2025å¹´06æœˆ07æ—¥ 18:53' âœ '2025-06-07 18:53:00'ï¼‰
        time_tag = soup.select_one("time.date")
        raw_date = time_tag.text.strip() if time_tag else ""
        try:
            date_obj = datetime.strptime(raw_date, "%Yå¹´%mæœˆ%dæ—¥ %H:%M")  # âœ… è§£æä¸­æ–‡æ—¥æœŸ
            date = date_obj.strftime("%Y-%m-%d %H:%M:%S")  # âœ… è½‰æ›ç‚ºæ¨™æº– MySQL DATETIME æ ¼å¼
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£ææ—¥æœŸï¼š{raw_date}ï¼ŒåŸå› ï¼š{e}")
            date = None  # æˆ–å¯ç”¨ç©ºå­—ä¸² date = ""

        # æ“·å–å…§æ–‡æ®µè½
        article_body = soup.select("div.story > p")
        content = "\n".join(p.text.strip() for p in article_body if p.text.strip())

        return {
            "title": title.text.strip() if title else "",
            "url": url,
            "date": date,  # âœ… ä½¿ç”¨å·²è½‰æ›æ ¼å¼çš„æ—¥æœŸ
            "content": content
        }

    except Exception as e:
        print(f"âŒ éŒ¯èª¤æ“·å–: {url}ï¼ŒåŸå› ï¼š{e}")
        return None

def crawl_ettoday(keyword, max_pages=3):
    """çˆ¬å– ETtoday æœå°‹çµæœï¼Œæ ¹æ“šé—œéµå­—èˆ‡æœ€å¤§é æ•¸"""
    results = []
    page = 1

    while True:
        search_url = f"https://www.ettoday.net/news_search/doSearch.php?keywords={keyword}&idx={page}"
        print(f"  ğŸ“„ æŠ“å–ç¬¬ {page} é ï¼š{search_url}")
        try:
            res = requests.get(search_url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = soup.select(".box_2 h2 a")  # æŠ“å–æ–°èæ¨™é¡Œé€£çµ

            if not articles:
                print(f"  âœ… æ²’æœ‰æ›´å¤šæ–‡ç« ï¼ŒçµæŸã€Œ{keyword}ã€")
                break

            for a in articles:
                article_url = a['href']
                data = parse_article(article_url)
                if data:
                    data['keyword'] = keyword
                    results.append(data)
                time.sleep(1)  # âœ… å‹å–„çˆ¬èŸ²ï¼Œé¿å…è§¸ç™¼é¢¨æ§

            page += 1
            if max_pages > 0 and page > max_pages:
                break

        except Exception as e:
            print(f"  âŒ ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            break

    return results
