# worker.py
import pika
import json
from et_crawler import crawl_ettoday
from db import insert_article

def callback(ch, method, properties, body):
    task = json.loads(body)
    keyword = task.get("keyword")
    max_pages = task.get("max_pages", 3)

    print(f"âœ… æ”¶åˆ°ä»»å‹™ï¼š{keyword}ï¼ˆæœ€å¤š {max_pages} é ï¼‰")
    articles = crawl_ettoday(keyword, max_pages)

    for article in articles:
        insert_article(article)
        print(f"ğŸ“ å·²å¯«å…¥ï¼š{article['title'][:20]}...")

connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))
channel = connection.channel()
channel.queue_declare(queue='crawl_tasks')

channel.basic_consume(queue='crawl_tasks', on_message_callback=callback, auto_ack=True)
print('ğŸ•“ ç­‰å¾…ä»»å‹™ä¸­ï¼ˆCtrl+C å¯ä¸­æ­¢ï¼‰...')
channel.start_consuming()
